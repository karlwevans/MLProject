# Data Science Guide
### Karl Evans Dec 2024

## Systems
### Gitlab
### Bash/Shell
htop \
pwd \
ls \
cd \
mkdir \
touch \
.. \
rm \
fm -rf \
\
Git \
git clone \
git pull \
git add \
git commit -m '' \
git push \
git branch \
gti status \
git checkout 


### Containerisation
#### - Enviroments
#### -- Conda
conda env list \
conda create --name <> python =<> \
conda activate \
conda deactivate \
conda env remove --name <> \
#### - Kernels

### Data Analysis
#### - SQL
#### - Data Structures

### Model Lifecycling
#### - MLflow

## 1.Preprocessing 
### EDA
### Pipeline
## Feature Engineering
#- Categorical
OneHotEncoding
LabelEncoding
FrequencyEncoding
MeanEncoding
TargetEncoding

#- Numeric
Smoothing
Clipping
Z-score scaling
Linear scaling
log scaling
# Binning
# - Good for 1) non-linear 2) clustered



ignore_features = ['reliability','intrnt_email_addr','clnt_intrnl_id']

cat_col = [col for col, type in df.dtypes.items() if type in ["category", 'object']and col not in ignore_features]
num_col = [col for col, type in df.dtypes.items() if type in ['float', 'int64']and col not in ignore_features]
features = num_col + cat_col

y=df['reliability']
X=df.drop(['reliability','intrnt_email_addr','clnt_intrnl_id'], axis=1)  df['reliability'] = np.where(df['reliability'] == "Unreliable", 1, 0)
y= df['reliability'].values

# Missing Data
Mean/Median Imputation
"-999" for tree based models
Most Frequent Imputation
"Miss"

# EG catboost
from catboost import CatBoostClassifier

def build_pipeline(num_features: List[str], cat_features: List[str]) -> Pipeline:
    """Full pipeline

    This function constructs the whole pipeline for training

    Params:
        config (Dict): Config content from yaml
        num_features (List[str]): List of numeric features
        cat_features (List[str]): List of categorial feature

    Returns:
        Pipeline that contians pre-process, sampling (If specified) and model

    Note:
        * Config assumes we're in the `pipeline` level already
    """
    numeric_transformer = make_pipeline(SimpleImputer(strategy='mean'),
                                       StandardScaler())

    categorical_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'),
        #SimpleImputer(strategy='constant', fill_value='missing'),
        OneHotEncoder(handle_unknown='ignore', min_frequency=0.05))

    preprocessor = make_column_transformer((numeric_transformer, num_features),
        (categorical_transformer, cat_features),
        remainder="passthrough")
    
    pipe = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', CatBoostClassifier())
    ])
    return pipe

pipeline = build_pipeline(num_features=num_col, cat_features=cat_col)

### Class Imbalance

#### - Oversampling

#### - Undersampling

### Hyperparameter Tuning
 import optuna
from sklearn.model_selection import cross_val_score
import numpy as np

X_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)

def objective(trial):
    # Suggest hyperparameters
    params = {
        'model__iterations':trial.suggest_int("iterations", , 500, step=1),
        'model__depth': trial.suggest_int("depth", 4, 10, step=1),,
        'model__learning_rate':trial.suggest_int("learning_rate", , 500, step=1),,
        'model__cat_features':'',
        'model__loss_function':'Logloss',
        'model__verbose': 'True'     
    }

iterations=2,
                           depth=2,
                           learning_rate=1,
                           loss_function='Logloss',
                           verbose=True)
    
    model = pipeline.set_params(**params)
    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1', verbose=3, error_score='raise')
    
    return scores.mean()
#### - GridSearchCV
#### - HyperOpt
#### - Optuna
  # Create a study object
study = optuna.create_study(direction="maximize")

# Optimize the objective function
study.optimize(objective, n_trials=10)
### Cross Validation
### Feature Importance
## 2. Supervised Learning
### Linear Regression
#### - Ridge
#### - Lasso
### Logistic Regression
### Support Vector Machines
### Randon Forests
### K Nearest Neighbours
### Naive Bayes
### Gradient Boosting
#### - XGBoost
### Neural Networks
### Outlier Detection
#### - Isolation Forest
#### - One Class Classification
from sklearn.datasets import load_iris
import sklearn
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, 
                                                    iris.target, 
                                                    test_size=0.4, 
                                                    random_state=1)
svm_class_1 = SVC()
svm_class_1.fit(X_train,y_train==1)
plot_classifier(X_train, y_train==1,svm_class_1)
## 3. Unsupervised Learning
### Clustering
#### - k-Means
#### - Hierarchical
#### - DBSCANS
#### - Gaussian Mixture Models
### Dimensionality Reduction
#### - PCA
#### - t-SNE
#### - LDA
#### - ICA
#### - UMAP
### Association
#### - 
## 4. Semi-Supervised Learning
## 5. Reinforcement Learning
## 6. Metrics
## 7. Other
### Causality
#### - Propensity
### Ensembling
#### - Bagging
#### - Boosting
#### - Stacking/Meta 
## 8. Production
#### 1. AP455.md
#### 2. aep_manifest
#### 3. entrypoint.sh
